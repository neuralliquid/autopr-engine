name: "Phase 2: Rapid Prototyping to Production"
description: "Automated workflow for converting rapid prototypes to production-ready applications"

triggers:
  - on: push
    branches: [main, develop]
    condition:
      "contains(github.event.head_commit.message, 'replit') ||
      contains(github.event.head_commit.message, 'lovable') ||
      contains(github.event.head_commit.message, 'bolt') ||
      contains(github.event.head_commit.message, 'same') ||
      contains(github.event.head_commit.message, 'emergent')"

  - on: repository_dispatch
    event_types: [prototype-export, replit-export, lovable-export, bolt-export]

  - on: workflow_dispatch
    inputs:
      platform_override:
        description: "Override platform detection"
        required: false
        type: choice
        options: ["auto-detect", "replit", "lovable", "bolt", "same", "emergent"]

      enhancement_level:
        description: "Enhancement level"
        required: true
        type: choice
        options: ["basic", "production_ready", "enterprise"]
        default: "production_ready"

inputs:
  enhance_for_production:
    type: boolean
    default: true
    description: "Automatically enhance prototype for production"

  target_platform:
    type: string
    default: "azure"
    description: "Target deployment platform"

  enable_monitoring:
    type: boolean
    default: true
    description: "Enable Azure SRE monitoring"

  create_staging:
    type: boolean
    default: true
    description: "Create staging environment"

  run_e2e_tests:
    type: boolean
    default: true
    description: "Run end-to-end tests"

steps:
  - name: "Collect Repository Information"
    id: collect_info
    description: "Gather repository files and commit information"
    uses: script
    with:
      script: |
        import os
        import json
        import subprocess
        from pathlib import Path

        # Get recent commit messages
        try:
            result = subprocess.run(['git', 'log', '--oneline', '-10'],
                                  capture_output=True, text=True)
            commit_messages = result.stdout.strip().split('\n')
        except:
            commit_messages = [github.event.head_commit.message] if github.event.head_commit else []

        # Read package.json if it exists
        package_json_content = None
        package_path = Path('package.json')
        if package_path.exists():
            with open(package_path, 'r') as f:
                package_json_content = f.read()

        # Collect key files
        key_files = {}
        for file_pattern in ['.replit', 'lovable.config.js', 'bolt.config.json', 'same.config.js', 'emergent.sh']:
            file_path = Path(file_pattern)
            if file_path.exists():
                with open(file_path, 'r') as f:
                    key_files[file_pattern] = f.read()

        return {
            'repository_url': f"https://github.com/{github.repository}",
            'commit_messages': commit_messages,
            'workspace_path': github.workspace,
            'package_json_content': package_json_content,
            'key_files': key_files
        }

  - name: "Detect Prototyping Platform"
    id: detect_platform
    description: "Identify which rapid prototyping platform was used"
    uses: ./actions/platform_detector.py
    with:
      repository_info: "${{ steps.collect_info.outputs.repository_info }}"
      commit_messages: "${{ steps.collect_info.outputs.commit_messages }}"
      key_files: "${{ steps.collect_info.outputs.key_files }}"

  - name: "Enhance Prototype for Production"
    id: enhance_prototype
    description: "Transform prototype into production-ready application"
    condition: "${{ inputs.enhance_for_production }}"
    uses: ./actions/prototype_enhancer.py
    with:
      platform: "${{ steps.detect_platform.outputs.platform }}"
      enhancement_level: "${{ inputs.enhancement_level }}"
      target_platform: "${{ inputs.target_platform }}"

  - name: "Run Quality Checks"
    id: quality_checks
    description: "Comprehensive quality analysis of enhanced code"
    uses: ./actions/quality_engine.py
    with:
      mode: "comprehensive"
      files: "${{ steps.enhance_prototype.outputs.enhanced_files }}"
      config: "pyproject.toml"

  - name: "Create Production Enhancement PR"
    id: create_pr
    description: "Create pull request with production enhancements"
    condition: "${{ steps.quality_checks.outputs.quality_score > 0.7 }}"
    uses: script
    with:
      script: |
        import json
        import os

        platform = "${{ steps.detect_platform.outputs.platform }}"
        enhanced_files = json.loads('${{ steps.enhance_prototype.outputs.enhanced_files }}')
        new_files = json.loads('${{ steps.enhance_prototype.outputs.new_files }}')
        quality_score = float("${{ steps.quality_checks.outputs.quality_score }}")

        pr_title = f"ðŸš€ {platform.title()} Prototype â†’ Production Ready"
        pr_body = f"""
        ## ðŸŽ¯ Production Enhancement Summary

        **Platform Detected:** {platform.title()}
        **Quality Score:** {quality_score:.1%}
        **Enhancement Level:** ${{{{ inputs.enhancement_level }}}}

        ### ðŸ“ Files Changed
        {format_file_changes(enhanced_files, new_files)}

        ### âœ… Quality Checks
        {format_quality_results()}

        ### ðŸŽ¯ Next Steps
        1. Review and test all changes
        2. Update environment variables
        3. Deploy to staging environment
        4. Run comprehensive test suite
        5. Monitor application performance

        ---
        *This PR was automatically generated by AutoPR Phase 2 workflow*
        """

        # Create the PR (in real implementation, this would use GitHub API)
        print(f"Creating PR: {pr_title}")
        print(pr_body)

        # Write enhanced files to workspace
        for file_path, content in enhanced_files.items():
            with open(file_path, 'w') as f:
                f.write(content)

        for file_path, content in new_files.items():
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, 'w') as f:
                f.write(content)

        return {
            'pr_title': pr_title,
            'pr_body': pr_body,
            'files_modified': len(enhanced_files),
            'files_created': len(new_files)
        }

        def format_file_changes(enhanced, new):
            changes = []

            if enhanced:
                changes.append("**Modified:**")
                for file in enhanced.keys():
                    changes.append(f"- ðŸ“ {file}")

            if new:
                changes.append("**Created:**")
                for file in new.keys():
                    changes.append(f"- âœ¨ {file}")

            return '\n'.join(changes)

        def format_quality_results():
            passed = json.loads('${{ steps.quality_checks.outputs.checks_passed }}')
            failed = json.loads('${{ steps.quality_checks.outputs.checks_failed }}')

            result = []

            if passed:
                result.append("**âœ… Passed:**")
                result.extend([f"- {check}" for check in passed])

            if failed:
                result.append("**âŒ Needs Attention:**")
                result.extend([f"- {check}" for check in failed])

            return '\n'.join(result)

  - name: "Deploy to Staging"
    id: deploy_staging
    description: "Deploy enhanced application to staging environment"
    condition: "${{ inputs.create_staging && steps.quality_checks.outputs.quality_score > 0.7 }}"
    uses: script
    with:
      script: |
        import json

        platform = "${{ steps.detect_platform.outputs.platform }}"
        deployment_config = json.loads('${{ steps.enhance_prototype.outputs.deployment_config }}')

        # Platform-specific deployment logic
        if platform == "replit":
            staging_url = deploy_replit_to_azure_staging(deployment_config)
        elif platform == "lovable":
            staging_url = deploy_lovable_to_vercel_staging(deployment_config)
        elif platform == "bolt":
            staging_url = deploy_bolt_to_azure_container_staging(deployment_config)
        else:
            staging_url = deploy_generic_to_azure_staging(deployment_config)

        return {
            'staging_url': staging_url,
            'deployment_config': deployment_config,
            'platform': platform
        }

        def deploy_replit_to_azure_staging(config):
            # Mock deployment to Azure App Service
            return f"https://replit-app-staging.azurewebsites.net"

        def deploy_lovable_to_vercel_staging(config):
            # Mock deployment to Vercel
            return f"https://lovable-app-staging.vercel.app"

        def deploy_bolt_to_azure_container_staging(config):
            # Mock deployment to Azure Container Instances
            return f"https://bolt-app-staging.azurecontainer.io"

        def deploy_generic_to_azure_staging(config):
            # Mock generic deployment
            return f"https://prototype-app-staging.azurewebsites.net"

  - name: "Run E2E Tests"
    id: e2e_tests
    description: "Run end-to-end tests on staging deployment"
    condition: "${{ inputs.run_e2e_tests && steps.deploy_staging.outputs.staging_url }}"
    uses: script
    with:
      script: |
        import json
        import time

        staging_url = "${{ steps.deploy_staging.outputs.staging_url }}"
        platform = "${{ steps.detect_platform.outputs.platform }}"

        # Platform-specific E2E test scenarios
        test_scenarios = {
            'replit': ['basic_functionality', 'api_endpoints', 'error_handling'],
            'lovable': ['component_rendering', 'responsive_design', 'accessibility'],
            'bolt': ['full_stack_flow', 'database_operations', 'api_integration'],
            'same': ['customization_features', 'branding_display', 'feature_toggles'],
            'emergent': ['automation_scripts', 'deployment_process', 'monitoring']
        }

        scenarios = test_scenarios.get(platform, ['basic_functionality'])
        test_results = []

        for scenario in scenarios:
            # Mock test execution
            print(f"Running E2E test: {scenario}")
            time.sleep(1)  # Simulate test execution

            # Mock test result (95% pass rate)
            passed = scenario != 'error_handling' or platform != 'replit'
            test_results.append({
                'scenario': scenario,
                'passed': passed,
                'duration': '30s',
                'details': f"Test {scenario} completed successfully" if passed else f"Test {scenario} failed"
            })

        # Calculate overall results
        passed_tests = sum(1 for t in test_results if t['passed'])
        total_tests = len(test_results)
        pass_rate = passed_tests / total_tests if total_tests > 0 else 0

        return {
            'test_results': test_results,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'pass_rate': pass_rate,
            'all_tests_passed': pass_rate == 1.0,
            'staging_url': staging_url
        }

  - name: "Platform-Specific Post-Processing"
    id: post_processing
    description: "Run platform-specific post-processing tasks"
    uses: script
    with:
      script: |
        import json

        platform = "${{ steps.detect_platform.outputs.platform }}"

        post_processing_tasks = []

        if platform == "replit":
            post_processing_tasks.extend([
                "Configure PM2 process management",
                "Setup Azure App Service continuous deployment",
                "Enable Application Insights monitoring"
            ])
        elif platform == "lovable":
            post_processing_tasks.extend([
                "Optimize React bundle size",
                "Setup Vercel Analytics",
                "Configure Lighthouse CI"
            ])
        elif platform == "bolt":
            post_processing_tasks.extend([
                "Setup database backup strategy",
                "Configure container health checks",
                "Enable API monitoring"
            ])
        elif platform == "same":
            post_processing_tasks.extend([
                "Document customization options",
                "Setup feature flag management",
                "Configure branding pipeline"
            ])
        elif platform == "emergent":
            post_processing_tasks.extend([
                "Validate automation scripts",
                "Setup infrastructure monitoring",
                "Configure backup and recovery"
            ])

        return {
            'platform': platform,
            'post_processing_tasks': post_processing_tasks,
            'tasks_completed': len(post_processing_tasks)
        }

  - name: "Send Notifications"
    id: notifications
    description: "Send team notifications about prototype enhancement"
    condition: "${{ steps.enhance_prototype.outputs.enhanced_files }}"
    uses: script
    with:
      script: |
        import json

        platform = "${{ steps.detect_platform.outputs.platform }}"
        quality_score = float("${{ steps.quality_checks.outputs.quality_score }}")
        staging_url = "${{ steps.deploy_staging.outputs.staging_url if steps.deploy_staging.outputs else '' }}"
        e2e_pass_rate = float("${{ steps.e2e_tests.outputs.pass_rate if steps.e2e_tests.outputs else '0' }}")

        # Send Slack notification
        slack_message = {
            "text": f"ðŸš€ {platform.title()} Prototype Enhanced for Production",
            "attachments": [
                {
                    "color": "good" if quality_score > 0.8 else "warning",
                    "fields": [
                        {
                            "title": "Platform",
                            "value": platform.title(),
                            "short": True
                        },
                        {
                            "title": "Quality Score",
                            "value": f"{quality_score:.1%}",
                            "short": True
                        },
                        {
                            "title": "Staging URL",
                            "value": staging_url or "Not deployed",
                            "short": False
                        },
                        {
                            "title": "E2E Tests",
                            "value": f"{e2e_pass_rate:.1%} passed" if e2e_pass_rate > 0 else "Not run",
                            "short": True
                        }
                    ]
                }
            ]
        }

        # Platform-specific channel notifications
        platform_channels = {
            'replit': '#replit-deployments',
            'lovable': '#frontend-deployments',
            'bolt': '#fullstack-deployments',
            'same': '#template-deployments',
            'emergent': '#devops-deployments'
        }

        target_channel = platform_channels.get(platform, '#development')

        print(f"Sending Slack notification to {target_channel}")
        print(json.dumps(slack_message, indent=2))

        return {
            'notification_sent': True,
            'target_channel': target_channel,
            'message_type': 'success' if quality_score > 0.8 else 'warning'
        }

outputs:
  platform_detected:
    description: "Detected prototyping platform"
    value: "${{ steps.detect_platform.outputs.platform }}"

  confidence_score:
    description: "Platform detection confidence"
    value: "${{ steps.detect_platform.outputs.confidence }}"

  enhancement_completed:
    description: "Whether enhancement was completed"
    value: "${{ steps.enhance_prototype.outputs.enhanced_files != '{}' }}"

  quality_score:
    description: "Overall quality score"
    value: "${{ steps.quality_checks.outputs.quality_score }}"

  staging_url:
    description: "Staging environment URL"
    value: "${{ steps.deploy_staging.outputs.staging_url }}"

  e2e_pass_rate:
    description: "End-to-end test pass rate"
    value: "${{ steps.e2e_tests.outputs.pass_rate }}"

  pr_created:
    description: "Production enhancement PR details"
    value: "${{ steps.create_pr.outputs.pr_title }}"

# Workflow metadata
metadata:
  version: "2.0.0"
  phase: "2"
  complexity: "medium-high"
  estimated_duration: "10-30 minutes"
  dependencies:
    - "Platform Detector"
    - "Prototype Enhancer"
    - "Azure App Service"
    - "Vercel (optional)"
    - "GitHub API"
  requirements:
    - "GITHUB_TOKEN"
    - "AZURE_SERVICE_PRINCIPAL"
    - "SLACK_WEBHOOK_URL"
    - "VERCEL_TOKEN (optional)"
