name: "Phase 2: Rapid Prototyping to Production"
description: "Automated workflow for converting rapid prototypes to production-ready applications"

triggers:
  - on: push
    branches: [main, develop]
    condition: "contains(github.event.head_commit.message, 'replit') || contains(github.event.head_commit.message, 'lovable') || contains(github.event.head_commit.message, 'bolt') || contains(github.event.head_commit.message, 'same') || contains(github.event.head_commit.message, 'emergent')"
  
  - on: repository_dispatch
    event_types: [prototype-export, replit-export, lovable-export, bolt-export]
  
  - on: workflow_dispatch
    inputs:
      platform_override:
        description: "Override platform detection"
        required: false
        type: choice
        options: ["auto-detect", "replit", "lovable", "bolt", "same", "emergent"]
      
      enhancement_level:
        description: "Enhancement level"
        required: true
        type: choice
        options: ["basic", "production_ready", "enterprise"]
        default: "production_ready"

inputs:
  enhance_for_production:
    type: boolean
    default: true
    description: "Automatically enhance prototype for production"
  
  target_platform:
    type: string
    default: "azure"
    description: "Target deployment platform"
  
  enable_monitoring:
    type: boolean
    default: true
    description: "Enable Azure SRE monitoring"
  
  create_staging:
    type: boolean
    default: true
    description: "Create staging environment"
  
  run_e2e_tests:
    type: boolean
    default: true
    description: "Run end-to-end tests"

steps:
  - name: "Collect Repository Information"
    id: collect_info
    description: "Gather repository files and commit information"
    uses: script
    with:
      script: |
        import os
        import json
        import subprocess
        from pathlib import Path
        
        # Get recent commit messages
        try:
            result = subprocess.run(['git', 'log', '--oneline', '-10'], 
                                  capture_output=True, text=True)
            commit_messages = result.stdout.strip().split('\n')
        except:
            commit_messages = [github.event.head_commit.message] if github.event.head_commit else []
        
        # Read package.json if it exists
        package_json_content = None
        package_path = Path('package.json')
        if package_path.exists():
            with open(package_path, 'r') as f:
                package_json_content = f.read()
        
        # Collect key files
        key_files = {}
        for file_pattern in ['.replit', 'lovable.config.js', 'bolt.config.json', 'same.config.js', 'emergent.sh']:
            file_path = Path(file_pattern)
            if file_path.exists():
                with open(file_path, 'r') as f:
                    key_files[file_pattern] = f.read()
        
        return {
            'repository_url': f"https://github.com/{github.repository}",
            'commit_messages': commit_messages,
            'workspace_path': github.workspace,
            'package_json_content': package_json_content,
            'key_files': key_files
        }

  - name: "Detect Prototyping Platform"
    id: detect_platform
    description: "Identify which rapid prototyping platform was used"
    uses: ./actions/platform_detector.py
    with:
      repository_url: "${{ steps.collect_info.outputs.repository_url }}"
      commit_messages: "${{ steps.collect_info.outputs.commit_messages }}"
      workspace_path: "${{ steps.collect_info.outputs.workspace_path }}"
      package_json_content: "${{ steps.collect_info.outputs.package_json_content }}"

  - name: "Platform Detection Results"
    id: platform_results
    description: "Process platform detection results and determine next steps"
    uses: script
    with:
      script: |
        import json
        
        platform = "${{ steps.detect_platform.outputs.detected_platform }}"
        confidence = float("${{ steps.detect_platform.outputs.confidence_score }}")
        override = "${{ inputs.platform_override }}"
        
        # Use override if specified
        if override and override != "auto-detect":
            final_platform = override
            confidence = 1.0
        else:
            final_platform = platform
        
        # Determine if we should proceed with enhancement
        should_enhance = confidence > 0.3 or override != "auto-detect"
        
        # Get platform-specific configuration
        platform_config = json.loads('${{ steps.detect_platform.outputs.platform_specific_config }}')
        
        return {
            'final_platform': final_platform,
            'confidence': confidence,
            'should_enhance': should_enhance,
            'platform_config': platform_config,
            'recommended_workflow': "${{ steps.detect_platform.outputs.recommended_workflow }}"
        }

  - name: "Enhance Prototype for Production"
    id: enhance_prototype
    description: "Enhance the prototype based on detected platform"
    condition: "${{ steps.platform_results.outputs.should_enhance && inputs.enhance_for_production }}"
    uses: ./actions/prototype_enhancer.py
    with:
      platform: "${{ steps.platform_results.outputs.final_platform }}"
      project_files: "${{ steps.collect_info.outputs.key_files }}"
      enhancement_type: "${{ inputs.enhancement_level }}"
      target_environment: "production"
      workspace_path: "${{ steps.collect_info.outputs.workspace_path }}"

  - name: "Run Quality Checks"
    id: quality_checks
    description: "Run platform-specific quality and security checks"
    condition: "${{ steps.enhance_prototype.outputs.enhanced_files }}"
    uses: script
    with:
      script: |
        import json
        import subprocess
        from pathlib import Path
        
        platform = "${{ steps.platform_results.outputs.final_platform }}"
        enhanced_files = json.loads('${{ steps.enhance_prototype.outputs.enhanced_files }}')
        new_files = json.loads('${{ steps.enhance_prototype.outputs.new_files }}')
        
        checks_passed = []
        checks_failed = []
        
        # Security check - ensure security dependencies are added
        if 'package.json' in enhanced_files:
            package_data = json.loads(enhanced_files['package.json'])
            security_deps = ['helmet', 'cors', 'express-rate-limit']
            
            for dep in security_deps:
                if dep in package_data.get('dependencies', {}):
                    checks_passed.append(f"Security dependency {dep} added")
                else:
                    checks_failed.append(f"Missing security dependency: {dep}")
        
        # Configuration check - ensure production configs exist
        required_configs = {
            'replit': ['.env.example', 'ecosystem.config.js'],
            'lovable': ['tsconfig.json', 'next.config.js'],
            'bolt': ['Dockerfile', 'docker-compose.yml'],
            'same': ['customization.config.js'],
            'emergent': ['deploy.sh', 'monitoring.sh']
        }
        
        for config_file in required_configs.get(platform, []):
            if config_file in new_files:
                checks_passed.append(f"Configuration file {config_file} generated")
            else:
                checks_failed.append(f"Missing configuration: {config_file}")
        
        # Test configuration check
        test_files = ['jest.config.js', 'playwright.config.js']
        test_files_found = sum(1 for f in test_files if f in new_files)
        
        if test_files_found > 0:
            checks_passed.append(f"Testing configuration added ({test_files_found} files)")
        else:
            checks_failed.append("No testing configuration found")
        
        # Overall quality score
        total_checks = len(checks_passed) + len(checks_failed)
        quality_score = len(checks_passed) / total_checks if total_checks > 0 else 0
        
        return {
            'checks_passed': checks_passed,
            'checks_failed': checks_failed,
            'quality_score': quality_score,
            'all_checks_passed': len(checks_failed) == 0,
            'critical_issues': [f for f in checks_failed if 'security' in f.lower()]
        }

  - name: "Create Production PR"
    id: create_pr
    description: "Create PR with production enhancements"
    condition: "${{ steps.enhance_prototype.outputs.enhanced_files }}"
    uses: script
    with:
      script: |
        import json
        import os
        
        platform = "${{ steps.platform_results.outputs.final_platform }}"
        enhanced_files = json.loads('${{ steps.enhance_prototype.outputs.enhanced_files }}')
        new_files = json.loads('${{ steps.enhance_prototype.outputs.new_files }}')
        enhancement_summary = "${{ steps.enhance_prototype.outputs.enhancement_summary }}"
        checklist = json.loads('${{ steps.enhance_prototype.outputs.production_checklist }}')
        quality_score = float("${{ steps.quality_checks.outputs.quality_score }}")
        
        # Create PR title and body
        pr_title = f"ðŸš€ Production Enhancement: {platform.title()} â†’ Azure Deployment"
        
        pr_body = f"""
## ðŸ¤– Automated Production Enhancement

**Platform Detected**: {platform.title()}
**Confidence**: {float("${{ steps.platform_results.outputs.confidence }}"):.1%}
**Quality Score**: {quality_score:.1%}

### ðŸ“‹ Enhancement Summary
{enhancement_summary}

### ðŸ“ Files Modified
{format_file_changes(enhanced_files, new_files)}

### âœ… Production Checklist
{format_checklist(checklist)}

### ðŸš¦ Quality Checks
{format_quality_results()}

### ðŸŽ¯ Next Steps
1. Review and test all changes
2. Update environment variables
3. Deploy to staging environment
4. Run comprehensive test suite
5. Monitor application performance

---
*This PR was automatically generated by AutoPR Phase 2 workflow*
        """
        
        # Create the PR (in real implementation, this would use GitHub API)
        print(f"Creating PR: {pr_title}")
        print(pr_body)
        
        # Write enhanced files to workspace
        for file_path, content in enhanced_files.items():
            with open(file_path, 'w') as f:
                f.write(content)
        
        for file_path, content in new_files.items():
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, 'w') as f:
                f.write(content)
        
        return {
            'pr_title': pr_title,
            'pr_body': pr_body,
            'files_modified': len(enhanced_files),
            'files_created': len(new_files)
        }
        
        def format_file_changes(enhanced, new):
            changes = []
            
            if enhanced:
                changes.append("**Modified:**")
                for file in enhanced.keys():
                    changes.append(f"- ðŸ“ {file}")
            
            if new:
                changes.append("**Created:**")
                for file in new.keys():
                    changes.append(f"- âœ¨ {file}")
            
            return '\n'.join(changes)
        
        def format_checklist(items):
            return '\n'.join([f"- [ ] {item}" for item in items])
        
        def format_quality_results():
            passed = json.loads('${{ steps.quality_checks.outputs.checks_passed }}')
            failed = json.loads('${{ steps.quality_checks.outputs.checks_failed }}')
            
            result = []
            
            if passed:
                result.append("**âœ… Passed:**")
                result.extend([f"- {check}" for check in passed])
            
            if failed:
                result.append("**âŒ Needs Attention:**")
                result.extend([f"- {check}" for check in failed])
            
            return '\n'.join(result)

  - name: "Deploy to Staging"
    id: deploy_staging
    description: "Deploy enhanced application to staging environment"
    condition: "${{ inputs.create_staging && steps.quality_checks.outputs.quality_score > 0.7 }}"
    uses: script
    with:
      script: |
        import json
        
        platform = "${{ steps.platform_results.outputs.final_platform }}"
        deployment_config = json.loads('${{ steps.enhance_prototype.outputs.deployment_config }}')
        
        # Platform-specific deployment logic
        if platform == "replit":
            staging_url = deploy_replit_to_azure_staging(deployment_config)
        elif platform == "lovable":
            staging_url = deploy_lovable_to_vercel_staging(deployment_config)
        elif platform == "bolt":
            staging_url = deploy_bolt_to_azure_container_staging(deployment_config)
        else:
            staging_url = deploy_generic_to_azure_staging(deployment_config)
        
        return {
            'staging_url': staging_url,
            'deployment_config': deployment_config,
            'platform': platform
        }
        
        def deploy_replit_to_azure_staging(config):
            # Mock deployment to Azure App Service
            return f"https://replit-app-staging.azurewebsites.net"
        
        def deploy_lovable_to_vercel_staging(config):
            # Mock deployment to Vercel
            return f"https://lovable-app-staging.vercel.app"
        
        def deploy_bolt_to_azure_container_staging(config):
            # Mock deployment to Azure Container Instances
            return f"https://bolt-app-staging.azurecontainer.io"
        
        def deploy_generic_to_azure_staging(config):
            # Mock generic deployment
            return f"https://prototype-app-staging.azurewebsites.net"

  - name: "Run E2E Tests"
    id: e2e_tests
    description: "Run end-to-end tests on staging deployment"
    condition: "${{ inputs.run_e2e_tests && steps.deploy_staging.outputs.staging_url }}"
    uses: script
    with:
      script: |
        import json
        import time
        
        staging_url = "${{ steps.deploy_staging.outputs.staging_url }}"
        platform = "${{ steps.platform_results.outputs.final_platform }}"
        
        # Platform-specific E2E test scenarios
        test_scenarios = {
            'replit': ['basic_functionality', 'api_endpoints', 'error_handling'],
            'lovable': ['component_rendering', 'responsive_design', 'accessibility'],
            'bolt': ['full_stack_flow', 'database_operations', 'api_integration'],
            'same': ['customization_features', 'branding_display', 'feature_toggles'],
            'emergent': ['automation_scripts', 'deployment_process', 'monitoring']
        }
        
        scenarios = test_scenarios.get(platform, ['basic_functionality'])
        test_results = []
        
        for scenario in scenarios:
            # Mock test execution
            print(f"Running E2E test: {scenario}")
            time.sleep(1)  # Simulate test execution
            
            # Mock test result (95% pass rate)
            passed = scenario != 'error_handling' or platform != 'replit'
            test_results.append({
                'scenario': scenario,
                'passed': passed,
                'duration': '30s',
                'details': f"Test {scenario} completed successfully" if passed else f"Test {scenario} failed"
            })
        
        # Calculate overall results
        passed_tests = sum(1 for t in test_results if t['passed'])
        total_tests = len(test_results)
        pass_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        return {
            'test_results': test_results,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'pass_rate': pass_rate,
            'all_tests_passed': pass_rate == 1.0,
            'staging_url': staging_url
        }

  - name: "Platform-Specific Post-Processing"
    id: post_processing
    description: "Run platform-specific post-processing tasks"
    uses: script
    with:
      script: |
        import json
        
        platform = "${{ steps.platform_results.outputs.final_platform }}"
        
        post_processing_tasks = []
        
        if platform == "replit":
            post_processing_tasks.extend([
                "Configure PM2 process management",
                "Setup Azure App Service continuous deployment",
                "Enable Application Insights monitoring"
            ])
        
        elif platform == "lovable":
            post_processing_tasks.extend([
                "Optimize React bundle size",
                "Setup Vercel Analytics",
                "Configure Lighthouse CI"
            ])
        
        elif platform == "bolt":
            post_processing_tasks.extend([
                "Setup database backup strategy",
                "Configure container health checks",
                "Enable API monitoring"
            ])
        
        elif platform == "same":
            post_processing_tasks.extend([
                "Document customization options",
                "Setup feature flag management",
                "Configure branding pipeline"
            ])
        
        elif platform == "emergent":
            post_processing_tasks.extend([
                "Validate automation scripts",
                "Setup infrastructure monitoring",
                "Configure backup and recovery"
            ])
        
        return {
            'platform': platform,
            'post_processing_tasks': post_processing_tasks,
            'tasks_completed': len(post_processing_tasks)
        }

  - name: "Send Notifications"
    id: notifications
    description: "Send team notifications about prototype enhancement"
    condition: "${{ steps.enhance_prototype.outputs.enhanced_files }}"
    uses: script
    with:
      script: |
        import json
        
        platform = "${{ steps.platform_results.outputs.final_platform }}"
        quality_score = float("${{ steps.quality_checks.outputs.quality_score }}")
        staging_url = "${{ steps.deploy_staging.outputs.staging_url if steps.deploy_staging.outputs else '' }}"
        e2e_pass_rate = float("${{ steps.e2e_tests.outputs.pass_rate if steps.e2e_tests.outputs else '0' }}")
        
        # Send Slack notification
        slack_message = {
            "text": f"ðŸš€ {platform.title()} Prototype Enhanced for Production",
            "attachments": [
                {
                    "color": "good" if quality_score > 0.8 else "warning",
                    "fields": [
                        {
                            "title": "Platform",
                            "value": platform.title(),
                            "short": True
                        },
                        {
                            "title": "Quality Score",
                            "value": f"{quality_score:.1%}",
                            "short": True
                        },
                        {
                            "title": "Staging URL",
                            "value": staging_url or "Not deployed",
                            "short": False
                        },
                        {
                            "title": "E2E Tests",
                            "value": f"{e2e_pass_rate:.1%} passed" if e2e_pass_rate > 0 else "Not run",
                            "short": True
                        }
                    ]
                }
            ]
        }
        
        # Platform-specific channel notifications
        platform_channels = {
            'replit': '#replit-deployments',
            'lovable': '#frontend-deployments', 
            'bolt': '#fullstack-deployments',
            'same': '#template-deployments',
            'emergent': '#devops-deployments'
        }
        
        target_channel = platform_channels.get(platform, '#development')
        
        print(f"Sending Slack notification to {target_channel}")
        print(json.dumps(slack_message, indent=2))
        
        return {
            'notification_sent': True,
            'target_channel': target_channel,
            'message_type': 'success' if quality_score > 0.8 else 'warning'
        }

outputs:
  platform_detected:
    description: "Detected prototyping platform"
    value: "${{ steps.platform_results.outputs.final_platform }}"
  
  confidence_score:
    description: "Platform detection confidence"
    value: "${{ steps.platform_results.outputs.confidence }}"
  
  enhancement_completed:
    description: "Whether enhancement was completed"
    value: "${{ steps.enhance_prototype.outputs.enhanced_files != '{}' }}"
  
  quality_score:
    description: "Overall quality score"
    value: "${{ steps.quality_checks.outputs.quality_score }}"
  
  staging_url:
    description: "Staging environment URL"
    value: "${{ steps.deploy_staging.outputs.staging_url }}"
  
  e2e_pass_rate:
    description: "End-to-end test pass rate"
    value: "${{ steps.e2e_tests.outputs.pass_rate }}"
  
  pr_created:
    description: "Production enhancement PR details"
    value: "${{ steps.create_pr.outputs.pr_title }}"

# Workflow metadata
metadata:
  version: "2.0.0"
  phase: "2"
  complexity: "medium-high"
  estimated_duration: "10-30 minutes"
  dependencies:
    - "Platform Detector"
    - "Prototype Enhancer"
    - "Azure App Service"
    - "Vercel (optional)"
    - "GitHub API"
  requirements:
    - "GITHUB_TOKEN"
    - "AZURE_SERVICE_PRINCIPAL"
    - "SLACK_WEBHOOK_URL"
    - "VERCEL_TOKEN (optional)" 